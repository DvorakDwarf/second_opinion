{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3674ae17",
   "metadata": {},
   "source": [
    "## Second Opinion model practice on MNIST\n",
    "\n",
    "Hello and welcome to my TED talk. This notebook is about an idea for architecture I got when learning DL. I am oblivious to how effective it is or even if I'm original. The idea sounds similar to MoE (Mixture of Experts). While MoE seems to be about seperating unique tasks between a couple models, my idea is to take MNIST and make 10 models that each are only responsible for their own number and nothing else. The idea is that they would be easier to tweak individually and theoretically improve accuracy. \\\n",
    "I also realised that this can be an Evangelion reference if you squint at it *really* hard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ed89b13",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "To make things simple hyperparameters and constants in general will be listed in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78b7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many threads to pull data\n",
    "DATA_WORKERS = 0\n",
    "\n",
    "#Model stuff\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "#Labels for second opinion models\n",
    "POSITIVE = [1., 0.]\n",
    "NEGATIVE = [0., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686e6bf",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "But first we need to initialize the victim of many amateur machine learning students - MNIST, a dataset of tens of thousands of pictures of handwritten digits that we will use to teach our \"\"\"experts\"\"\" how to recognize numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3991cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#We will use target_transform soon\n",
    "def get_loaders(target_transform=None):\n",
    "    #No data augmentation necessary. It's literally just 28x28 pixels\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    train_data = datasets.MNIST(root='data', \n",
    "                                train=True,\n",
    "                                download=True, \n",
    "                                transform=transform,\n",
    "                                target_transform=target_transform)\n",
    "    #Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_workers=DATA_WORKERS,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    val_data = datasets.MNIST(root='data', \n",
    "                                train=False,\n",
    "                                download=True, \n",
    "                                transform=transform,\n",
    "                                target_transform=target_transform)\n",
    "    #Data loader\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_workers=DATA_WORKERS)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "target_transform = transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "train_loader, val_loader = get_loaders(target_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45bc0383",
   "metadata": {},
   "source": [
    "Now let's take a look at the data just to make sure we didn't horribly mess up so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247b8553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "image_batch, labels = next(data_iter)\n",
    "image_batch = image_batch.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(7,7), nrows=3, ncols=3, sharey=True, sharex=True)\n",
    "for ax, img in zip(axes.flatten(), image_batch):\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87e8e514",
   "metadata": {},
   "source": [
    "# The Fun Stuff™\n",
    "\n",
    "Now we can get to architecture. For this particular experiment I will use a very shallow CNN. I'm not trying to get state of the art performance here so it's nice to not have to overthink things"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81c3a53f",
   "metadata": {},
   "source": [
    "# The Baseline \n",
    "\n",
    "This will be our CNN in question. We will use this solo model as the baseline on which to judge the second opinion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d92472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Solo_Expert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Solo_Expert, self).__init__()        \n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(1, 16, kernel_size=3),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, kernel_size=3),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )        \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 10)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)        \n",
    "        x = self.flatten(x)       \n",
    "        output = self.fc1(x)\n",
    "        return output    \n",
    "    \n",
    "#Check what device to use\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "solo_model = Solo_Expert().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5accfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm \n",
    "import datetime\n",
    "\n",
    "target = \"full\"\n",
    "\n",
    "optimizer = optim.SGD(solo_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "def training_loop(model, target, train_loader, val_loader, quiet=False):\n",
    "    print(f\"Beginning of training for target: {target}\")\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        best_loss = 9999\n",
    "        \n",
    "        #Train\n",
    "        for (imgs, labels) in tqdm(train_loader, desc=\"Training\", disable=quiet):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            out = model(imgs)\n",
    "            loss = loss_fn(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        #Validate\n",
    "        for (val_imgs, val_labels) in tqdm(val_loader, desc=\"Validation\", disable=quiet):\n",
    "            val_imgs = val_imgs.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            model.train(True)\n",
    "\n",
    "            val_out = model(val_imgs)\n",
    "            val_loss = loss_fn(val_out, val_labels)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        epoch_val_loss = total_val_loss / len(val_loader)\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "            \n",
    "        #Save the best model\n",
    "        if epoch_val_loss < best_loss:\n",
    "            best_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), \"data/\" + f\"MNIST_{target}.pth\")\n",
    "\n",
    "        # if epoch == 1 or epoch % 10 == 0:\n",
    "        now = datetime.datetime.now()\n",
    "        if quiet == True:\n",
    "            pass\n",
    "            # if epoch % 5 == 0:\n",
    "            #     print(f\"{now}\\nEpoch {epoch}\\ntr_loss {epoch_loss:.5}\\nval_loss {epoch_val_loss:.5}\\n\")\n",
    "        else:\n",
    "            print(f\"{now}\\nEpoch {epoch}\\ntr_loss {epoch_loss:.5}\\nval_loss {epoch_val_loss:.5}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(solo_model, target, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aba99295",
   "metadata": {},
   "source": [
    "And now to test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10a59bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "        \n",
    "    for name, loader in [(\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                # print(\"Out: \" + str(outputs[0]))\n",
    "                # print(\"Truth: \" + str(labels[0]) + \"\\n\")\n",
    "\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                _, truth = torch.max(labels, dim=1)\n",
    "                \n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == truth).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70f61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: 0.93\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "solo_model.load_state_dict(torch.load(f\"data/MNIST_full_[2023-05-20 14:06:48.644023].pth\"))\n",
    "\n",
    "#One hot encode\n",
    "target_transform = transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "train_loader, val_loader = get_loaders(target_transform)\n",
    "\n",
    "validate(solo_model, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7f9f6ab",
   "metadata": {},
   "source": [
    "An ~93% accuracy might not be the knife's edge in terms of classification but it's a fair start."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db7b6412",
   "metadata": {},
   "source": [
    "# Second Opinion\n",
    "\n",
    "Now that we have seen the performance of the baseline, we can compare it to a small horde of single-minded models. We will start by creating our own transform for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68981ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLOW ?\n",
    "class Relabel:\n",
    "    def __init__(self, target):\n",
    "        target = torch.zeros(10, dtype=torch.float).scatter_(\n",
    "            dim=0, index=torch.tensor(target), value=1)\n",
    "\n",
    "        self.target = target\n",
    "    \n",
    "    def __call__(self, label):\n",
    "        if torch.equal(label, self.target) == True:\n",
    "            return torch.Tensor(POSITIVE)\n",
    "        else:\n",
    "            return torch.Tensor(NEGATIVE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e4b316a",
   "metadata": {},
   "source": [
    "Let's make a somewhat lobotomized version of our original expert that is a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af46f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Originally tried with less features than the original but it was bad\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Expert, self).__init__()        \n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(1, 16, kernel_size=3),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, kernel_size=3),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )        \n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 2)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)        \n",
    "        x = self.flatten(x)       \n",
    "        output = self.fc1(x)\n",
    "        return output    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c488ac",
   "metadata": {},
   "source": [
    "Now let's modify the training for this hive mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0237f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of training for target: 0\n",
      "Accuracy val: 0.99\n",
      "Beginning of training for target: 1\n",
      "Accuracy val: 0.99\n",
      "Beginning of training for target: 2\n",
      "Accuracy val: 0.98\n",
      "Beginning of training for target: 3\n",
      "Accuracy val: 0.98\n",
      "Beginning of training for target: 4\n",
      "Accuracy val: 0.98\n",
      "Beginning of training for target: 5\n",
      "Accuracy val: 0.97\n",
      "Beginning of training for target: 6\n",
      "Accuracy val: 0.99\n",
      "Beginning of training for target: 7\n",
      "Accuracy val: 0.98\n",
      "Beginning of training for target: 8\n",
      "Accuracy val: 0.98\n",
      "Beginning of training for target: 9\n",
      "Accuracy val: 0.97\n"
     ]
    }
   ],
   "source": [
    "for target in range(0, 10):\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    Relabel(target)\n",
    "    ])\n",
    "\n",
    "    train_loader, val_loader = get_loaders(target_transform)\n",
    "    hiveling = Expert()\n",
    "    optimizer = optim.SGD(hiveling.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    training_loop(model=hiveling,\n",
    "                target=target,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                quiet=True)\n",
    "\n",
    "    validate(hiveling, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06110e6b",
   "metadata": {},
   "source": [
    "Creating a class to contain all 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db6694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hive(nn.Module):\n",
    "    def __init__(self, hive_models):\n",
    "        super(Hive, self).__init__()        \n",
    "\n",
    "        self.hive_models = hive_models\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Data loader might not have enough data for a full batch\n",
    "        current_batch_size = x.shape[0]\n",
    "\n",
    "        outputs = torch.zeros((current_batch_size, 10))\n",
    "        for index, hiveling in enumerate(hive_models):\n",
    "            out = hiveling(x)\n",
    "\n",
    "            #Combining the output of all models into the correct shape\n",
    "            outputs[:, index] = out[:, 0]\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "480a2de6",
   "metadata": {},
   "source": [
    "Let's evaluate them all at once now and see if this did anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58abb975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: 0.92\n"
     ]
    }
   ],
   "source": [
    "hive_models = []\n",
    "\n",
    "target_transform = transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "train_loader, val_loader = get_loaders(target_transform)\n",
    "\n",
    "for target in range(0, 10):\n",
    "    hiveling = Expert()\n",
    "    # soo_model.load_state_dict(torch.load(f\"data/MNIST_full_[2023-05-20 13:37:37.751415].pth\"))\n",
    "    hiveling.load_state_dict(torch.load(f\"data/MNIST_{target}.pth\"))\n",
    "    hive_models.append(hiveling)\n",
    "\n",
    "hive = Hive(hive_models)\n",
    "\n",
    "validate(hive, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5db6085d",
   "metadata": {},
   "source": [
    "As well as individual performance if you forgot results from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d577d37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy val: 0.99\n",
      "Accuracy val: 0.99\n",
      "Accuracy val: 0.98\n",
      "Accuracy val: 0.98\n",
      "Accuracy val: 0.98\n",
      "Accuracy val: 0.97\n",
      "Accuracy val: 0.99\n",
      "Accuracy val: 0.98\n",
      "Accuracy val: 0.98\n",
      "Accuracy val: 0.97\n"
     ]
    }
   ],
   "source": [
    "for target in range(0, 10):\n",
    "    # print(target)\n",
    "    target_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    Relabel(target)\n",
    "    ])\n",
    "\n",
    "    train_loader, val_loader = get_loaders(target_transform)\n",
    "    hiveling = Expert()\n",
    "\n",
    "    hiveling.load_state_dict(torch.load(f\"data/MNIST_{target}.pth\"))\n",
    "\n",
    "    validate(hiveling, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6472385",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Unfortunately it did not work out that well. The overall performance tends to be the same or lower, at the cost massively slower training. There is still some potential use from this. If the classes were vastly different in difficulty creating a model tuned for the difficult problem might increase accuracy and prevent overfitting on the other classes. It might be enough if you are looking to juice out that extra half a percent of accuracy from the model and don't mind waiting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
