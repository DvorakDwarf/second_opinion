{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3674ae17",
   "metadata": {},
   "source": [
    "## Second Opinion model practice on MNIST\n",
    "\n",
    "Hello and welcome to my TED talk. This notebook is about an idea for architecture I got when learning DL. I am oblivious to how effective it is or even if I'm original. The idea sounds similar to MoE (Mixture of Experts). While MoE seems to be about seperating unique tasks between a couple models, my idea is to take MNIST and make 10 models that each are only responsible for their own number and nothing else. The idea is that they would be easier to tweak individually and theoretically improve accuracy. \\\n",
    "I also realised that this can be an Evangelion reference if you squint at it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686e6bf",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "But first we need to initialize the victim of many amateur machine learning students - MNIST, a dataset of tens of thousands of pictures of handwritten digits that we will use to teach our \"\"\"experts\"\"\" how to recognize numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3991cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "DATA_WORKERS = 0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#We will use target_transform soon\n",
    "def get_loaders(target_transform=None):\n",
    "    #No data augmentation necessary. It's literally just 28x28 pixels\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    train_data = datasets.MNIST(root='data', \n",
    "                                train=True,\n",
    "                                download=True, \n",
    "                                transform=transform,\n",
    "                                target_transform=target_transform)\n",
    "    #Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_workers=DATA_WORKERS,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    val_data = datasets.MNIST(root='data', \n",
    "                                train=False,\n",
    "                                download=True, \n",
    "                                transform=transform,\n",
    "                                target_transform=target_transform)\n",
    "    #Data loader\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, \n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            num_workers=DATA_WORKERS)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "target_transform = transforms.Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    "train_loader, val_loader = get_loaders(target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc0383",
   "metadata": {},
   "source": [
    "Now let's take a look at what we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "image_batch, labels = next(data_iter)\n",
    "image_batch = image_batch.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(7,7), nrows=3, ncols=3, sharey=True, sharex=True)\n",
    "for ax, img in zip(axes.flatten(), image_batch):\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63036faf",
   "metadata": {},
   "source": [
    "If you spent any amount of time trying to do image classification these numbers better be burned in your mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8e514",
   "metadata": {},
   "source": [
    "# The Fun Stuffâ„¢\n",
    "\n",
    "Now we can get to architecture. For this particular experiment I will be going back to the good ol' days of dense(fully connected) layers. I'm not trying to get state of the art performance here so it's nice to not have to overthink things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3a53f",
   "metadata": {},
   "source": [
    "# The Baseline \n",
    "\n",
    "We will begin with creating a regular fully connected classifier for MNIST and see how it performs. We will use this as the baseline on which to judge the second opinion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2d92472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Solo_Expert(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Solo_Expert, self).__init__()\n",
    "        \n",
    "        # define hidden linear layers\n",
    "        self.fc1 = nn.Linear(28*28, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n",
    "        \n",
    "        # final fully-connected layer\n",
    "        self.fc4 = nn.Linear(hidden_dim*4, 10)\n",
    "        \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer with tanh applied\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return out\n",
    "    \n",
    "#Check what device to use\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"mps\" if use_mps else \"cpu\")\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "HIDDEN_DIM = 32\n",
    "solo_model = Solo_Expert(HIDDEN_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5accfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm \n",
    "import datetime\n",
    "\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "target = \"full\"\n",
    "\n",
    "optimizer = optim.SGD(solo_model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "def training_loop(model, target, train_loader, val_loader):\n",
    "    beginning = datetime.datetime.now()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        best_loss = 9999\n",
    "        \n",
    "        #Train\n",
    "        for (imgs, labels) in tqdm(train_loader, desc=\"Training\"):\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            out = solo_model(imgs)\n",
    "            loss = loss_fn(out, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        #Validate\n",
    "        for (val_imgs, val_labels) in tqdm(val_loader, desc=\"Validation\"):\n",
    "            val_imgs = val_imgs.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "\n",
    "            model.train(True)\n",
    "\n",
    "            val_out = model(val_imgs)\n",
    "            val_loss = loss_fn(val_out, val_labels)\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "        epoch_val_loss = total_val_loss / len(val_loader)\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "            \n",
    "        if epoch_val_loss < best_loss:\n",
    "            best_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), \"data/\" + f\"MNIST_[{beginning}].pth\")\n",
    "\n",
    "        # if epoch == 1 or epoch % 10 == 0:\n",
    "        now = datetime.datetime.now()\n",
    "        print(f\"{now}\\nEpoch {epoch}\\ntr_loss {epoch_loss:.5}\\nval_loss {epoch_val_loss:.5}\\n\")\n",
    "\n",
    "training_loop(solo_model, target, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aba99295",
   "metadata": {},
   "source": [
    "And now to test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c70f61d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.8376e-01, -4.9722e+00, -2.9358e+00, -2.4496e-01,  1.7727e-01,\n",
      "          1.9353e+00, -5.2895e+00,  6.5521e+00, -1.9683e-01,  3.7434e+00],\n",
      "        [ 2.2175e+00, -3.1020e+00,  6.6008e+00,  3.3791e+00, -4.9611e+00,\n",
      "          3.2751e+00,  3.3950e+00, -7.5959e+00,  2.3853e+00, -6.2375e+00],\n",
      "        [-4.9105e+00,  5.3028e+00,  1.5899e+00,  1.3726e+00, -1.2044e+00,\n",
      "         -3.9618e-01, -6.3773e-01, -4.0844e-01,  1.4526e+00, -3.8401e-01],\n",
      "        [ 8.8345e+00, -1.5461e+01,  1.8741e+00,  1.0435e-01, -1.9261e+00,\n",
      "          5.6794e+00,  1.6438e-01, -1.3470e+00,  4.6945e-01, -2.0585e+00],\n",
      "        [-7.9220e-01, -5.5725e+00, -7.6428e-01, -3.0193e+00,  3.8786e+00,\n",
      "          4.1368e-01,  7.3784e-01,  6.5906e-01,  9.5891e-01,  2.8883e+00],\n",
      "        [-5.9699e+00,  6.3069e+00,  1.3965e+00,  1.9979e+00, -1.6289e+00,\n",
      "         -3.5793e-01, -1.6468e+00,  1.7519e-01,  1.8942e+00, -5.7006e-02],\n",
      "        [-2.5279e+00, -3.6300e+00, -2.6497e+00, -1.6065e+00,  3.5247e+00,\n",
      "          8.1324e-01, -1.6441e+00,  2.1116e+00,  1.5241e+00,  4.2160e+00],\n",
      "        [-4.4174e+00, -1.2757e+00, -1.6355e+00, -1.4584e+00,  3.1189e+00,\n",
      "          1.7385e-01, -8.7707e-01,  1.4934e+00,  1.9066e+00,  3.7239e+00],\n",
      "        [ 2.2115e-01, -7.0635e+00,  3.3600e+00, -3.0676e+00,  2.2977e+00,\n",
      "          1.1618e+00,  4.8649e+00, -3.5091e+00,  1.5970e+00, -6.7404e-01],\n",
      "        [-2.1945e+00, -7.5873e+00, -4.2506e+00, -3.5138e+00,  5.1586e+00,\n",
      "          1.1844e+00, -3.3602e+00,  6.3265e+00,  1.0356e+00,  7.0027e+00],\n",
      "        [ 8.0863e+00, -9.4409e+00,  3.1665e+00,  4.0448e+00, -5.3964e+00,\n",
      "          5.5219e+00, -1.2689e+00, -4.4062e+00,  1.5826e+00, -4.6326e+00],\n",
      "        [-1.2396e-01, -4.0522e+00,  3.5357e+00,  3.5830e-02, -2.7561e-01,\n",
      "          1.6545e+00,  3.0407e+00, -4.4473e+00,  2.4201e+00, -2.0667e+00],\n",
      "        [-2.0502e+00, -6.6552e+00, -3.1737e+00, -3.4457e+00,  5.0432e+00,\n",
      "          7.7282e-01, -1.7197e+00,  3.7685e+00,  1.3060e+00,  5.8124e+00],\n",
      "        [ 6.8385e+00, -1.1664e+01,  1.6309e-01,  9.3918e-01, -1.6100e+00,\n",
      "          4.8587e+00, -1.8652e+00, -8.4081e-01,  1.1835e+00, -6.7520e-01],\n",
      "        [-6.7489e+00,  7.0700e+00,  2.1070e+00,  2.6737e+00, -2.2973e+00,\n",
      "         -6.4306e-02, -1.1889e+00, -8.2949e-01,  2.4287e+00, -7.0449e-01],\n",
      "        [ 1.6616e+00, -2.7204e+00,  8.2609e-01,  3.4309e+00, -3.1045e+00,\n",
      "          3.0963e+00, -1.9657e+00, -1.8304e+00,  1.8674e+00, -1.7285e+00],\n",
      "        [-1.4830e+00, -6.0766e+00, -2.0799e+00, -2.9449e+00,  4.0883e+00,\n",
      "          6.2861e-01, -1.2715e+00,  2.8480e+00,  1.2608e+00,  4.6298e+00],\n",
      "        [ 2.4175e+00, -6.2086e+00, -2.5031e+00,  6.8164e-01, -1.0700e+00,\n",
      "          2.8829e+00, -5.5152e+00,  5.9236e+00, -1.2246e-01,  2.7191e+00],\n",
      "        [-1.9123e+00, -1.7346e+00,  2.0792e+00,  1.3716e+00, -9.4651e-01,\n",
      "          1.9424e+00,  1.3005e+00, -2.9539e+00,  2.4168e+00, -1.2630e+00],\n",
      "        [-1.8896e+00, -4.8149e+00, -2.0683e+00, -3.0275e+00,  4.7491e+00,\n",
      "          3.1901e-01,  2.7821e-01,  1.0766e+00,  1.0460e+00,  3.8844e+00],\n",
      "        [-1.8793e+00, -5.3167e+00, -4.8668e+00, -5.5745e-01,  2.2783e+00,\n",
      "          2.1007e+00, -6.1948e+00,  7.0017e+00,  1.3446e+00,  6.3622e+00],\n",
      "        [-5.3642e-01, -5.3877e+00,  3.1487e+00, -2.0107e+00,  2.1586e+00,\n",
      "          1.4190e+00,  5.9183e+00, -5.7928e+00,  1.8979e+00, -1.6110e+00],\n",
      "        [-3.7098e+00, -1.5472e+00,  2.5019e+00, -3.0262e+00,  2.9782e+00,\n",
      "         -7.1805e-01,  4.2346e+00, -1.7780e+00,  1.2043e+00,  4.4199e-01],\n",
      "        [ 1.4158e+00, -5.9081e+00,  5.4975e-01,  3.9468e-01,  5.0293e-02,\n",
      "          2.7967e+00,  5.4035e-01, -2.4651e+00,  1.9886e+00, -3.5428e-01],\n",
      "        [-1.4231e+00, -3.4275e+00, -1.4940e+00, -2.1945e+00,  3.2643e+00,\n",
      "          1.9011e-01, -4.1024e-01,  1.6805e+00,  5.4460e-01,  3.0862e+00],\n",
      "        [ 1.2379e+01, -2.0540e+01,  3.7788e+00, -1.0776e+00, -1.5535e+00,\n",
      "          6.6805e+00,  2.7277e+00, -4.1362e+00,  3.4800e-01, -3.8220e+00],\n",
      "        [ 5.0552e-02, -3.7608e+00, -2.8537e+00, -8.0779e-01,  1.1580e+00,\n",
      "          1.1781e+00, -3.9368e+00,  5.3974e+00, -2.5332e-01,  3.7172e+00],\n",
      "        [-1.3075e+00, -6.9674e+00, -1.7956e+00, -3.6410e+00,  5.3098e+00,\n",
      "          7.1741e-01,  8.8761e-01,  5.1817e-01,  1.4501e+00,  3.9617e+00],\n",
      "        [ 8.4675e+00, -1.1958e+01,  1.8251e+00,  3.8579e+00, -4.9887e+00,\n",
      "          6.3295e+00, -2.6755e+00, -2.5907e+00,  1.9369e+00, -3.1055e+00],\n",
      "        [-4.9778e+00,  4.5894e+00,  1.8084e+00,  1.7001e+00, -1.3060e+00,\n",
      "          8.4017e-02, -1.6884e-01, -1.3544e+00,  2.0327e+00, -6.9281e-01],\n",
      "        [-1.1944e+00, -3.9128e-01, -8.5973e-01,  5.1009e+00, -3.8443e+00,\n",
      "          3.2221e+00, -5.5553e+00,  1.6101e+00,  2.4082e+00,  2.7711e-01],\n",
      "        [-4.5068e+00,  4.1280e+00,  5.8768e-01,  1.4677e+00, -9.2145e-01,\n",
      "         -1.8807e-02, -1.3188e+00,  1.8547e-01,  1.5178e+00,  3.7893e-01],\n",
      "        [-5.3465e-01, -7.6243e-01,  8.5019e-01,  5.2635e+00, -3.9391e+00,\n",
      "          3.4546e+00, -2.9265e+00, -2.7453e+00,  3.3672e+00, -1.7226e+00],\n",
      "        [ 3.9140e+00, -9.1590e+00,  2.9673e+00, -2.9399e+00,  1.7173e+00,\n",
      "          1.8751e+00,  4.6514e+00, -3.5361e+00,  1.5823e-01, -1.6649e+00],\n",
      "        [-2.3749e+00, -1.6105e+00, -9.1863e-01,  1.2516e+00, -9.0474e-01,\n",
      "          1.3236e+00, -5.1099e+00,  4.6404e+00,  1.8875e+00,  2.9269e+00],\n",
      "        [ 2.1571e+00, -2.9858e+00,  6.3800e+00,  2.7073e+00, -4.4429e+00,\n",
      "          2.6291e+00,  2.7603e+00, -6.0586e+00,  1.8632e+00, -5.5455e+00],\n",
      "        [ 4.8089e-01, -4.7788e+00, -1.5969e+00,  2.8543e-03, -7.5106e-02,\n",
      "          1.8270e+00, -4.0330e+00,  4.6532e+00,  4.2213e-01,  2.8127e+00],\n",
      "        [-6.3540e+00,  6.0560e+00,  1.2438e+00,  1.6924e+00, -1.1649e+00,\n",
      "         -3.2064e-01, -1.0855e+00, -2.5833e-01,  2.1181e+00,  1.6316e-01],\n",
      "        [-2.6999e-01,  8.2675e-01,  4.4543e+00,  4.4939e+00, -4.9879e+00,\n",
      "          2.4214e+00,  3.1830e-01, -4.9003e+00,  2.5245e+00, -4.4983e+00],\n",
      "        [-7.0557e+00,  7.0412e+00,  2.2253e+00,  2.9688e+00, -2.4290e+00,\n",
      "          1.0047e-01, -1.2746e+00, -1.3081e+00,  3.0261e+00, -8.0392e-01],\n",
      "        [-3.9356e+00,  4.3966e+00,  1.1915e+00,  1.2551e+00, -1.0462e+00,\n",
      "         -2.1212e-01, -5.8818e-01, -4.7509e-01,  1.1738e+00, -3.5491e-01],\n",
      "        [-6.7039e-01, -3.8172e+00, -2.2007e+00, -7.3869e-01,  9.0900e-01,\n",
      "          1.1610e+00, -4.0245e+00,  5.4911e+00,  2.1338e-01,  3.7681e+00],\n",
      "        [-5.7307e+00, -1.5755e+00, -1.8612e+00, -2.9958e+00,  4.9173e+00,\n",
      "         -6.7841e-01, -5.7040e-01,  2.1761e+00,  2.1759e+00,  5.0816e+00],\n",
      "        [-3.1550e+00,  2.4067e+00,  3.2553e+00,  9.8899e-01, -1.2018e+00,\n",
      "          1.7445e-01,  1.6856e+00, -3.2329e+00,  2.0216e+00, -1.9156e+00],\n",
      "        [-1.9988e+00,  5.8015e-01,  2.3717e+00,  2.9511e+00, -2.6477e+00,\n",
      "          1.8698e+00, -1.3859e-01, -2.6836e+00,  2.4387e+00, -2.0073e+00],\n",
      "        [ 1.7514e+00, -3.9787e+00,  6.1846e-01,  3.3920e+00, -2.5911e+00,\n",
      "          3.5080e+00, -1.6432e+00, -2.8720e+00,  2.5766e+00, -1.5116e+00],\n",
      "        [-4.9804e+00,  3.5623e+00,  1.2984e+00,  2.2734e+00, -1.4639e+00,\n",
      "          6.1566e-01, -1.1536e+00, -1.0140e+00,  2.6448e+00, -1.3515e-01],\n",
      "        [-1.7132e+00, -1.2092e+00,  3.3655e+00, -1.0723e+00,  5.7656e-01,\n",
      "          1.0035e-01,  3.3862e+00, -3.0412e+00,  1.3264e+00, -1.3690e+00],\n",
      "        [-2.9945e+00, -8.8650e+00, -4.3153e+00, -3.5811e+00,  6.7809e+00,\n",
      "          1.5681e+00, -1.7848e+00,  2.6349e+00,  2.8999e+00,  7.1608e+00],\n",
      "        [-2.0782e+00, -5.6303e+00, -9.0661e-01, -4.2560e+00,  5.4603e+00,\n",
      "         -2.6311e-01,  1.5785e+00,  9.6988e-01,  9.3368e-01,  3.7943e+00],\n",
      "        [ 1.7063e+00, -6.5230e+00,  3.3749e+00, -1.3314e+00,  7.3160e-01,\n",
      "          2.1057e+00,  5.1210e+00, -5.1899e+00,  1.1700e+00, -2.3657e+00],\n",
      "        [ 4.6264e-01, -4.1516e+00,  9.8867e-01,  3.7792e+00, -3.1342e+00,\n",
      "          3.8871e+00, -2.2487e+00, -1.4598e+00,  2.8298e+00, -1.1634e+00],\n",
      "        [ 2.3571e+00, -7.9616e+00, -1.6622e+00, -3.5059e-01,  1.0206e+00,\n",
      "          3.1609e+00, -1.3445e+00,  5.2857e-01,  1.0181e+00,  1.5524e+00],\n",
      "        [ 4.1816e-01, -2.0117e+00,  1.5790e-01,  2.1652e+00, -1.4637e+00,\n",
      "          2.2125e+00, -1.3488e+00, -1.4117e+00,  1.6880e+00, -6.0143e-01],\n",
      "        [-1.4363e+00, -3.0322e+00,  4.2634e+00, -1.5838e+00,  8.6239e-01,\n",
      "          5.4521e-01,  4.8545e+00, -4.2505e+00,  1.7141e+00, -1.8758e+00],\n",
      "        [ 5.4287e+00, -1.0422e+01,  7.1375e-01,  1.2911e+00, -1.4050e+00,\n",
      "          4.6679e+00, -6.4261e-01, -2.8689e+00,  2.1201e+00, -1.2375e+00],\n",
      "        [-2.8669e-01, -8.2969e+00, -9.4653e-01, -3.8199e+00,  5.2508e+00,\n",
      "          1.0540e+00,  2.0583e+00, -7.2320e-01,  1.4668e+00,  3.0245e+00],\n",
      "        [-4.7887e+00,  5.4500e+00,  1.1592e+00,  2.0349e+00, -1.7016e+00,\n",
      "         -1.4841e-01, -1.6207e+00,  4.5401e-02,  1.5917e+00, -2.4443e-01],\n",
      "        [-1.8036e+00, -7.5413e+00, -3.8354e+00, -3.9502e+00,  5.7941e+00,\n",
      "          7.9039e-01, -1.8205e+00,  4.4545e+00,  9.0669e-01,  6.4328e+00],\n",
      "        [ 1.0233e+00, -3.7417e+00, -8.1831e-01, -2.3123e-01,  3.3853e-01,\n",
      "          1.5458e+00, -1.3605e+00,  1.3813e+00,  2.6631e-01,  1.0075e+00],\n",
      "        [-5.4278e-01, -3.6586e+00, -3.1265e+00,  4.4765e-03,  6.0357e-01,\n",
      "          1.6431e+00, -4.8236e+00,  5.8517e+00,  1.6323e-01,  3.8963e+00],\n",
      "        [-1.5597e-01, -6.1911e+00,  3.2467e+00, -9.3668e-01,  5.5767e-01,\n",
      "          1.9288e+00,  2.6229e+00, -3.6750e+00,  3.1108e+00, -8.7516e-01],\n",
      "        [-1.6672e+00, -3.0642e+00, -5.3020e-01, -1.5697e+00,  2.4904e+00,\n",
      "          5.4342e-01,  3.5006e-01, -5.8391e-02,  1.3830e+00,  2.0526e+00],\n",
      "        [-2.2099e+00,  1.8145e-01,  3.2612e+00,  2.3529e+00, -2.2076e+00,\n",
      "          1.4957e+00,  5.9381e-01, -3.5900e+00,  3.0604e+00, -2.1211e+00]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m                 correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((predicted \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum())\n\u001b[1;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name , correct \u001b[39m/\u001b[39m total))\n\u001b[0;32m---> 21\u001b[0m validate(solo_model, val_loader)\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m         _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m         total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((predicted \u001b[39m==\u001b[39;49m labels)\u001b[39m.\u001b[39msum())\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name , correct \u001b[39m/\u001b[39m total))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "solo_model.load_state_dict(torch.load(f\"data/MNIST_[2023-05-19 20:01:21.259617].pth\"))\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    for name, loader in [(\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(imgs)\n",
    "                print(outputs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(solo_model, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7f9f6ab",
   "metadata": {},
   "source": [
    "An 86-89% accuracy might not be the knife's edge in terms of classification but it's a fair start (for a dense network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db7b6412",
   "metadata": {},
   "source": [
    "# Second Opinion\n",
    "\n",
    "Now that we have seen the performance of the baseline, we can compare it to a small horde of single-minded models. A layer has been removed from these models because trying to classify whether something is 7 or not should not require as many parameters as distinguishing between all 10 numbers. We will start by creating our own transform for labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68981ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLOW ?\n",
    "class Relabel:\n",
    "    def __init__(self, target):\n",
    "        self.target = target\n",
    "    \n",
    "    def __call__(self, label):\n",
    "        if label == target:\n",
    "            label = torch.int64(0)\n",
    "        else:\n",
    "            label = torch.int64(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "814e5552",
   "metadata": {},
   "source": [
    "And we will make aslightly lobotomized model for the simpler task (overfitting be damned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, hidden_dim*2)\n",
    "        self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        #Overfitting be damned\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # all hidden layers\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        # final layer with tanh applied\n",
    "        out = F.tanh(self.fc3(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4c488ac",
   "metadata": {},
   "source": [
    "Now let's modify the training for this hive mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ca339",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in range(0, 10):\n",
    "    # print(target)\n",
    "    target_transform = Relabel(target)\n",
    "    train_loader, val_loader = get_loaders(target_transform)\n",
    "    hiveling = Expert(HIDDEN_DIM)\n",
    "\n",
    "    for _, y in train_loader:\n",
    "        print(y)\n",
    "\n",
    "    training_loop(model=hiveling,\n",
    "                target=target,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23eea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
